{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to prepare/see results of Cross-validation for open- and closed-cell St"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pyhdf.SD import SD, SDC \n",
    "from scipy import stats\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import patches as mpl_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "#version 2\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/tkurihana/scratch-midway2/data/clouds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch:        \n",
    "    def __init__(self, date, isOpen, thirtyFive, zeroTwo, label=None, feature=None, has_coord=False, coords=None):\n",
    "        self.date = date\n",
    "        self.isOpen = isOpen\n",
    "        self.thirtyFive = thirtyFive\n",
    "        self.zeroTwo = zeroTwo\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.has_coord = has_coord\n",
    "        self.coords = coords\n",
    "        \n",
    "    def print_attr(self):\n",
    "        print(\"date: \" + self.date)\n",
    "        print(\"isOpen: \" + str(self.isOpen))\n",
    "        print(\"label: \" + str(self.label))\n",
    "        if len(self.coords) > 0:\n",
    "            print(\"coords: \")\n",
    "            for i in self.coords:\n",
    "                print(str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Closed cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatadir = \"/project2/foster/clouds/src_analysis/labeled_clouds/close_cells_mod02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpatches_list = []\n",
    "cfilelist= glob.glob(os.path.join(cdatadir, \"*.npy\"))\n",
    "for filename in cfilelist:\n",
    "    tmp = np.load(filename)\n",
    "    for patch in tmp:\n",
    "        #print( len(np.where(np.isnan(patch))[0]) )\n",
    "        if len(np.where(np.isnan(patch))[0]) == 0:\n",
    "            cpatches_list.append(patch)\n",
    "cpatches = np.squeeze(np.concatenate(cpatches_list, axis=0), axis=(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 128, 128, 6)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpatches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cpatches_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Open cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "odatadir = \"/project2/foster/clouds/src_analysis/labeled_clouds/open_cells_mod02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" one file is only [8] dimension\n",
    "\"\"\"\n",
    "opatches_list = []\n",
    "ofilelist = glob.glob(os.path.join(odatadir, \"*.npy\"))\n",
    "for filename in ofilelist:\n",
    "    a = np.load(filename, allow_pickle=True)\n",
    "    if a.size > 100:\n",
    "        for patch in a:\n",
    "            if len(np.where(np.isnan(patch))[0]) == 0:\n",
    "                opatches_list.append(patch)\n",
    "opatches = np.squeeze(np.concatenate(opatches_list, axis=0), axis=(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 128, 128, 6)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opatches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "del opatches_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Patches\n",
    "patches = np.concatenate([cpatches, opatches], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 128, 128, 6)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make label: 0; closed, 1; open cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctest = np.zeros((cpatches.shape[0]))\n",
    "otest = np.ones((opatches.shape[0]))\n",
    "label = np.concatenate([ctest, otest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237,)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: 1-time Valiadtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(patches, label, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 128, 128, 6), (142,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n,h,w,c = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train.reshape(n, h*w*c), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9157894736842105"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,h,w,c = X_test.shape\n",
    "clf.score(X_test.reshape(n, h*w*c), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "n,h,w,c = patches.shape\n",
    "scores = cross_val_score(clf, patches.reshape(n, h*w*c), label, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 0.9375    , 0.82978723, 0.85106383, 0.85106383])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X, y, cv=5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96658312, 1.        , 0.96658312, 0.96658312, 1.        ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: Cross-validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [2.18492413 2.4025774  1.95280457 1.84609723 2.21522784]\n",
      "score_time [0.89239097 1.02013183 0.76489997 0.78053904 0.86005235]\n",
      "test_precision_macro [0.83333333 0.93826087 0.83181818 0.85740741 0.88709677]\n",
      "test_recall_macro [0.83333333 0.9375     0.83061594 0.84963768 0.84782609]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, patches.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice: Repeate Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "for train_index, test_index in ss.split(np.arange(0, patches.shape[0],1)):\n",
    "    #print(\"%s %s\" % (train_index, test_index))\n",
    "    X_train = patches[train_index]\n",
    "    Y_test  = patches[test_index]\n",
    "    x_label = label[train_index]\n",
    "    y_label = label[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use autoencoder and check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_model(model_dir, mtype):\n",
    "    #TODO add restart model dir and restart argument?\n",
    "    latest = 0, None\n",
    "    # get trained wegiht \n",
    "    for m in os.listdir(model_dir):\n",
    "        if \".h5\" in m and mtype in m:\n",
    "            epoch = int(m.split(\"-\")[1].replace(\".h5\", \"\"))\n",
    "            latest = max(latest, (epoch, m))\n",
    "\n",
    "    epoch, model_file = latest\n",
    "\n",
    "    if not os.listdir(model_dir):\n",
    "        raise NameError(\"no directory. check model path again\")\n",
    "\n",
    "    print(\" Load {} at {} epoch\".format(mtype, epoch))\n",
    "    model_def = model_dir+'/'+mtype+'.json'\n",
    "    model_weight = model_dir+'/'+mtype+'-'+str(epoch)+'.h5'\n",
    "    with open(model_def, \"r\") as f:\n",
    "        model = tf.keras.models.model_from_json(f.read())\n",
    "    model.load_weights(model_weight)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RI autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load encoder at 100 epoch\n"
     ]
    }
   ],
   "source": [
    "# 67011582; Best model\n",
    "model_datadir = '/home/tkurihana/rotate_invariant/stepbystep/transform/output_model'\n",
    "expname = 67011582\n",
    "model_dir = os.path.join(model_datadir,str(expname) )\n",
    "encoder = load_latest_model(model_dir, mtype='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 32, 32, 6)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change patch size\n",
    "height = width = 32\n",
    "rpatches_tf = tf.image.resize_images(patches, (height, width))\n",
    "rpatches_tf = tf.cast(rpatches_tf, tf.float64)\n",
    "rpatches = tf.keras.backend.eval(rpatches_tf)\n",
    "\n",
    "# standardization\n",
    "nmin = np.amin(rpatches, axis=(0,1,2))\n",
    "nmax = np.amax(rpatches, axis=(0,1,2))\n",
    "rpatches = (rpatches - nmin)/(nmax - nmin)\n",
    "rpatches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [0.02354741 0.02398419 0.02123642 0.01896548 0.02294421]\n",
      "score_time [0.01252246 0.01094294 0.00980878 0.00849438 0.01057386]\n",
      "test_precision_macro [0.81304348 0.94444444 0.83181818 0.78909091 0.92857143]\n",
      "test_recall_macro [0.8125     0.9375     0.83061594 0.78804348 0.91304348]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [0.0869801  0.09412098 0.09649682 0.0819664  0.08466935]\n",
      "score_time [0.07198858 0.05639362 0.06329393 0.04183602 0.05602431]\n",
      "test_precision_macro [0.25       0.24468085 0.25531915 0.25531915 0.25531915]\n",
      "test_recall_macro [0.5        0.47916667 0.5        0.5        0.5       ]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(gamma=2, random_state=0)\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [247.19460917 304.24902368]\n",
      "score_time [0.17841578 0.12321925]\n",
      "test_precision_macro [0.91360505 0.89105339]\n",
      "test_recall_macro [0.90706215 0.88936782]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "#clf =  GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf =  MLPClassifier(alpha=1, max_iter=1000)\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=2, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [276.62142754 353.07320738 346.02472138 275.18099546 289.90711761]\n",
      "score_time [0.02053118 0.09346724 0.00628734 0.00721383 0.0796268 ]\n",
      "test_precision_macro [0.87023593 0.96153846 0.85144928 0.79537037 0.91849817]\n",
      "test_recall_macro [0.85416667 0.95833333 0.85144928 0.78894928 0.91394928]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "#clf =  GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf =  MLPClassifier(alpha=1, max_iter=1000)\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [0.01461339 0.01359177 0.01640391 0.00810218 0.00860667]\n",
      "score_time [0.00445437 0.00429702 0.0045228  0.0026834  0.00292873]\n",
      "test_precision_macro [0.89652174 0.93826087 0.77819549 0.89454545 0.92857143]\n",
      "test_recall_macro [0.89583333 0.9375     0.76811594 0.89311594 0.91304348]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf =  RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [1.92635083 2.44037914 2.39723921 2.35921383 2.44672012]\n",
      "score_time [0.02276087 0.02757215 0.02309465 0.02349663 0.02296019]\n",
      "test_precision_macro [0.91666667 0.94444444 0.90092593 0.89454545 0.9137931 ]\n",
      "test_recall_macro [0.91666667 0.9375     0.89221014 0.89311594 0.89130435]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf =  AdaBoostClassifier()\n",
    "encs = encoder.predict(rpatches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NRI autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Load encoder at 100000 epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:791: UserWarning: models is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# 'm2_02_global_2000_2018_band28_29_31'\n",
    "model_datadir = '/home/tkurihana/rotate_invariant/stepbystep/transform/output_model'\n",
    "expname = 'm2_02_global_2000_2018_band28_29_31'\n",
    "model_dir = os.path.join(model_datadir,str(expname) )\n",
    "nriencoder = load_latest_model(model_dir, mtype='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [0.12828207 0.15790749 0.12065506 0.1190412  0.13650012]\n",
      "score_time [0.05376101 0.07097888 0.05228615 0.05123711 0.0618856 ]\n",
      "test_precision_macro [0.9021164  0.96153846 0.87545788 0.83971774 0.9137931 ]\n",
      "test_recall_macro [0.89583333 0.95833333 0.87137681 0.80525362 0.89130435]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "encs = nriencoder.predict(patches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [ 10.70810747 766.75788379]\n",
      "score_time [0.41641521 0.47076082]\n",
      "test_precision_macro [0.92253521 0.91666667]\n",
      "test_recall_macro [0.90677966 0.89655172]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "#clf =  GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf =  MLPClassifier(alpha=1, max_iter=1000)\n",
    "encs = nriencoder.predict(patches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=2, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tkurihana/.conda/envs/tf-cpu/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [813.8486774  596.98655963  22.70966363 927.69674039  20.7441051 ]\n",
      "score_time [0.01212454 0.01099968 0.022084   0.01797223 0.01471686]\n",
      "test_precision_macro [0.92857143 0.98       0.92857143 0.85392157 0.89454545]\n",
      "test_recall_macro [0.91666667 0.97916667 0.91304348 0.82699275 0.89311594]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "#clf =  GaussianProcessClassifier(1.0 * RBF(1.0))\n",
    "clf =  MLPClassifier(alpha=1, max_iter=1000)\n",
    "encs = nriencoder.predict(patches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [0.02940154 0.02922893 0.02957559 0.02761221 0.02761197]\n",
      "score_time [0.0088613  0.00885057 0.00910378 0.00872827 0.0088532 ]\n",
      "test_precision_macro [0.85555556 0.91958042 0.87545788 0.746337   0.8843985 ]\n",
      "test_recall_macro [0.83333333 0.91666667 0.87137681 0.74365942 0.87047101]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf =  RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "encs = nriencoder.predict(patches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time [9.44690204 8.9894042  9.40780926 9.28659058 9.74264002]\n",
      "score_time [0.02756834 0.03544092 0.0301137  0.03513455 0.03478169]\n",
      "test_precision_macro [0.9        0.96153846 0.80887681 0.77037037 0.86877395]\n",
      "test_recall_macro [0.875      0.95833333 0.80887681 0.76449275 0.84873188]\n"
     ]
    }
   ],
   "source": [
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf =  AdaBoostClassifier()\n",
    "encs = nriencoder.predict(patches)\n",
    "n,h,w,c = encs.shape\n",
    "scores = cross_validate(clf, encs.reshape(n, h*w*c), label, cv=5, scoring=scoring)\n",
    "sorted(scores.keys())\n",
    "for ikey in ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']:\n",
    "    print(ikey, scores[ikey])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
