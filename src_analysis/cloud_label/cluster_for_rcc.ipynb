{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation analysis against open/closed cells\n",
    "---------------\n",
    "This notebook is an adapted version of `visualize_patches.ipynb` for the purpose of clustering my labeled patches with the 80k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Load module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patheffects as PathEffects\n",
    "from math import ceil,sqrt\n",
    "from sklearn.manifold import TSNE\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## directory where your put lib_hdfs\n",
    "libdir='/home/rubywerman/clouds/src_analysis/lib_hdfs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1,os.path.join(sys.path[0],libdir)) # this line helps you to use your own functions in another directory\n",
    "from alignment_lib import _gen_patches\n",
    "from alignment_lib import const_clouds_array\n",
    "from alignment_lib import gen_mod02_img_sigle,  gen_mod35_img_single\n",
    "from alignment_lib import mod02_proc_sds_single\n",
    "from alignment_lib import _gen_patches\n",
    "from alignment_lib import const_clouds_array\n",
    "from analysis_lib import *\n",
    "from lib_datesinfo_ruby import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "homedir = '/home/rubywerman/scratch-midway2/lib_hdfs'\n",
    "datadir = homedir+\"/model/m2_02_global_2000_2018_band28_29_31\"\n",
    "step = 100000 # DONOT change so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 09:49:44.039654 140625153623872 deprecation.py:506] From /home/rubywerman/.conda/envs/clouds/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0807 09:49:44.041643 140625153623872 deprecation.py:506] From /home/rubywerman/.conda/envs/clouds/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "/home/rubywerman/.conda/envs/clouds/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:891: UserWarning: models is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n",
      "W0807 09:49:44.202463 140625153623872 deprecation.py:506] From /home/rubywerman/.conda/envs/clouds/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0807 09:49:44.424167 140625153623872 deprecation_wrapper.py:119] From /scratch/midway2/tkurihana/clouds/reproduction/models.py:64: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_def = datadir+'/encoder.json'\n",
    "encoder_weight = datadir+'/encoder-'+str(step)+'.h5'\n",
    "with open(encoder_def, \"r\") as f:\n",
    "    encoder = tf.keras.models.model_from_json(f.read())\n",
    "encoder.load_weights(encoder_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load labelled open/closed cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the date files of your labeled patches here. You can get these files from running and labeling patches in cloud_labeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the name of the directory containing your dates files, mod02, and mod35 data for your labeled patches\n",
    "filesdir = \"/home/rubywerman/clouds/src_analysis/labeled_data/class_patch_data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are patch and cluster objects that make it easy to access and write new data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch:        \n",
    "    def __init__(self, date, isOpen, thirtyFive, zeroTwo, label=None, feature=None, has_coord=False, coords=None):\n",
    "        self.date = date\n",
    "        self.isOpen = isOpen\n",
    "        self.thirtyFive = thirtyFive\n",
    "        self.zeroTwo = zeroTwo\n",
    "        self.label = label\n",
    "        self.feature = feature\n",
    "        self.has_coord = has_coord\n",
    "        self.coords = coords\n",
    "        \n",
    "    def print_attr(self):\n",
    "        print(\"date: \" + self.date)\n",
    "        print(\"isOpen: \" + str(self.isOpen))\n",
    "        print(\"label: \" + str(self.label))\n",
    "        if len(self.coords) > 0:\n",
    "            print(\"coords: \")\n",
    "            for i in self.coords:\n",
    "                print(str(i))   \n",
    "        \n",
    "class Cluster:\n",
    "    def __init__(self, label, patches=None, means=None, std=None, num_open=0):\n",
    "        self.label = label\n",
    "        self.patches = patches\n",
    "        self.means = means\n",
    "        self.std = std\n",
    "        self.num_open = num_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to load in your list of patch objects from `cloud_labeling.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_patch_list = np.load(filesdir + '072219.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean faulty patches\n",
    "class_patch_list = [patch for patch in class_patch_list if type(patch.zeroTwo) is not list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following file contains Tak's 80K patches\n",
    "mypath1 = \"/project2/foster/clouds/analysis/output_clouds_feature_2000_2018_validfiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(mypath1 + \"*.npz\")\n",
    "patches = []\n",
    "for f in files:\n",
    "    data = np.load(f)\n",
    "    lst = data.files\n",
    "    #data[lst[0]] contains cloud patches np.array(#of patches, 128) \n",
    "    patches.append(data[lst[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all cloud patch np.arrays into one np.array\n",
    "all_patches = patches[0]\n",
    "for p in patches[1:500]:\n",
    "    all_patches = np.concatenate((all_patches, p), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Analysis\n",
    "\n",
    "we will use a type of hierarchical clusering called `Agglometative clustering` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to donwload agglomerative [sklearn aggl](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)  \n",
    "\n",
    "`from sklearn.cluster import AgglomerativeClustering`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encs_list = []\n",
    "for patch in class_patch_list:\n",
    "    i = patch.zeroTwo\n",
    "    if type(i) is not list:\n",
    "        ix, iy = i.shape[:2]\n",
    "        encs = encoder.predict(i.reshape(ix * iy, 128,128,6))\n",
    "        encs_list += [encs.mean(axis=(1,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(encs_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features.shape)  # make sure, the shape is [#number of patches, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_features = np.concatenate((features, all_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N in [2, inf), you can change this number but save the result differently\n",
    "num_clusters = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turns any NAN values to 0 so code doesn't crash\n",
    "cleaned_features = np.nan_to_num(both_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate clustering data\n",
    "label = clustering.fit_predict(cleaned_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/home/rubywerman/clouds/src_analysis/cloud_label/all_label.npy', label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
